# ğŸ’¬ Interactive RAG Chat

Um assistente de cÃ³digo inteligente que responde perguntas sobre seu projeto Python usando RAG (Retrieval-Augmented Generation).

## ğŸ¯ **O que faz?**

- **Analisa seu cÃ³digo** Python e cria um Ã­ndice inteligente
- **Responde perguntas** sobre arquitetura, implementaÃ§Ã£o e uso
- **Busca contexto relevante** automaticamente
- **Interface colorida** e amigÃ¡vel no terminal
- **Suporte a mÃºltiplos LLMs** (Ollama local, Google Gemini)

## ğŸš€ **Como usar**

### InÃ­cio RÃ¡pido
```bash
# Navegue para a pasta do app
cd apps/interactive-chat

# Execute com configuraÃ§Ã£o padrÃ£o
python main.py
```

### OpÃ§Ãµes AvanÃ§adas
```bash
# Usar modelo mais potente
python main.py --model llama3.2:3b

# Carregar cÃ³digo de outro projeto
python main.py --path /caminho/para/projeto

# Usar Google Gemini
python main.py --llm gemini --model gemini-1.5-flash

# Modo silencioso
python main.py --quiet
```

### Argumentos DisponÃ­veis
- `--path, -p`: Caminho para cÃ³digo fonte (default: ../../src)
- `--pattern, -f`: PadrÃ£o de arquivos (default: **/*.py)
- `--llm, -l`: Tipo de LLM (ollama, gemini)
- `--model, -m`: Nome do modelo (default: llama3.2:1b)
- `--temperature, -t`: Temperatura de geraÃ§Ã£o (default: 0.1)
- `--max-docs, -d`: Max documentos por consulta (default: 3)
- `--quiet, -q`: Modo silencioso

## ğŸ® **Comandos no Chat**

Durante o chat, vocÃª pode usar estes comandos:

- `help` - Mostra todos os comandos
- `examples` - Exemplos de perguntas
- `stats` - EstatÃ­sticas do sistema
- `files` - Lista arquivos carregados
- `search` - Busca por palavras-chave
- `reload` - Recarrega documentos
- `info` - InformaÃ§Ãµes detalhadas
- `quit` - Sair do chat

## ğŸ’¡ **Exemplos de Perguntas**

- "Como funciona o PythonLoader?"
- "Qual a diferenÃ§a entre GeminiLLM e OllamaLLM?"
- "Como implementar um novo loader?"
- "Explique o padrÃ£o Factory usado no projeto"
- "Como o RAGEngine funciona?"

## ğŸ”§ **PrÃ©-requisitos**

### Para Ollama (Recomendado)
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo
ollama pull llama3.2:1b

# Iniciar servidor
ollama serve
```

### Para Google Gemini
```bash
# Configurar chave da API
export GOOGLE_API_KEY="sua_chave_aqui"

# Ou criar arquivo .env
echo "GOOGLE_API_KEY=sua_chave_aqui" > .env
```

## ğŸ“ **Estrutura do App**

```
interactive-chat/
â”œâ”€â”€ main.py              # ğŸ¯ Aplicativo principal
â”œâ”€â”€ code_assistant.py    # ğŸ¤– Assistente RAG principal
â”œâ”€â”€ chat_interface.py    # ğŸ’¬ Interface de chat
â”œâ”€â”€ search_engine.py     # ğŸ” Motor de busca
â”œâ”€â”€ colored_output.py    # ğŸ¨ UtilitÃ¡rio de cores
â””â”€â”€ README.md           # ğŸ“– Esta documentaÃ§Ã£o
```

## ğŸ› ï¸ **Arquitetura**

### Fluxo Principal
1. **Carregamento**: PythonLoader lÃª arquivos Python
2. **IndexaÃ§Ã£o**: SimpleSearchEngine cria Ã­ndice por palavras-chave
3. **Chat**: ChatInterface gerencia interaÃ§Ã£o do usuÃ¡rio
4. **Busca**: Encontra documentos relevantes para pergunta
5. **GeraÃ§Ã£o**: LLM gera resposta baseada no contexto
6. **ApresentaÃ§Ã£o**: ColoredOutput exibe resultado formatado

### Componentes
- **CodeAssistant**: Orquestra todo o pipeline RAG
- **SearchEngine**: Busca simples mas eficaz por relevÃ¢ncia
- **ChatInterface**: UX rica com comandos e exemplos
- **ColoredOutput**: Interface visual atrativa

## ğŸ¨ **PersonalizaÃ§Ã£o**

### Modificar Exemplos
Edite `chat_interface.py`, seÃ§Ã£o `example_questions`:
```python
self.example_questions = [
    "Sua pergunta customizada aqui",
    # ... mais exemplos
]
```

### Adicionar Comandos
No `ChatInterface.__init__()`:
```python
self.commands['meucomando'] = self._meu_comando

def _meu_comando(self):
    # Sua implementaÃ§Ã£o
    pass
```

### Customizar Motor de Busca
Modifique `SimpleSearchEngine.keyword_mapping` para ajustar relevÃ¢ncia.

## ğŸš€ **Performance**

- **llama3.2:1b**: ~2-5s por resposta, qualidade boa
- **llama3.2:3b**: ~5-10s por resposta, qualidade superior  
- **gemini-1.5-flash**: ~1-3s, mas requer internet e quota

## ğŸ“ **Logs e Debug**

Para debug detalhado, remova `--quiet`:
```bash
python main.py --path ../../src
```

Para erros de conexÃ£o:
```bash
# Verificar Ollama
curl http://localhost:11434/api/tags

# Verificar modelos instalados
ollama list
```

## ğŸ¤ **Contribuindo**

1. **Novos LLMs**: Adicione em `../src/llms/`
2. **Novos Loaders**: Adicione em `../src/loaders/`
3. **Melhorias na Interface**: Modifique `chat_interface.py`
4. **Motor de Busca**: Evolua `search_engine.py`

---

ğŸ’¡ **Dica**: Comece com o modelo `llama3.2:1b` para testes rÃ¡pidos, depois evolua para modelos mais potentes conforme necessÃ¡rio!