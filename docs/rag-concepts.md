# RAG (Retrieval-Augmented Generation) - Conceitos

*Em portuguÃªs - GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o*

## Em Resumo

O RAG Ã© uma tÃ©cnica que turbina a InteligÃªncia Artificial, tornando-a mais confiÃ¡vel, precisa e Ãºtil para tarefas do dia a dia, desde responder a perguntas de clientes em um site atÃ© auxiliar profissionais em pesquisas complexas.

## Uma Analogia

RAG: Dando uma "cola" para a InteligÃªncia Artificial

Imagine que vocÃª estÃ¡ fazendo uma prova muito difÃ­cil. VocÃª estudou bastante, mas nÃ£o se lembra de todos os detalhes. Agora, imagine que o professor permite que vocÃª consulte um livro especÃ­fico, apenas com as informaÃ§Ãµes relevantes para a prova, antes de responder cada pergunta. Suas respostas seriam muito mais precisas e completas, certo?

De forma simples, Ã© exatamente isso que o RAG (Retrieval-Augmented Generation, ou GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o) faz com a InteligÃªncia Artificial. Em vez de depender apenas do que "estudou" (seu treinamento original), a IA pode consultar uma fonte de informaÃ§Ãµes externa e atualizada antes de gerar uma resposta para vocÃª.

## O que o RAG resolve?

O RAG tem como proposta resolver alguns dos principais desafios dos grandes modelos de linguagem:

- **Evita "alucinaÃ§Ãµes":** Reduz as chances de a IA inventar informaÃ§Ãµes, pois ela baseia suas respostas em dados concretos.

- **MantÃ©m a IA atualizada:** Permite que a IA tenha acesso a informaÃ§Ãµes recentes, sem a necessidade de passar por um processo de treinamento completo e caro cada vez que novos dados surgem.

- **Aumenta a confianÃ§a:** Ao fornecer respostas baseadas em fontes especÃ­ficas, os sistemas com RAG podem, inclusive, citar de onde tiraram a informaÃ§Ã£o, permitindo que os usuÃ¡rios verifiquem a veracidade dos dados.

## Como funciona na prÃ¡tica?

Temos 2 grandes pontos a entender aqui: **IndexaÃ§Ã£o** e **ConversaÃ§Ã£o**.

### IndexaÃ§Ã£o

```mermaid
graph TD
    subgraph Processo de IndexaÃ§Ã£o
        A[/ğŸ’» CÃ³digo Fonte / Documentos/] -->|1 - Quebrar em pedaÃ§os lÃ³gicos| B[ğŸ“„ PedaÃ§os - Chunks];
        B -->|2 - Criar 'impressÃ£o digital' numÃ©rica| C(ğŸ§  Modelo de Embedding de IA);
        C -->|Para cada pedaÃ§o| D[#ï¸âƒ£ Vetores NumÃ©ricos];
        B -->|Texto Original| E;
        D -->|ImpressÃ£o Digital| E[(ğŸ’¾ Banco de Dados Vetorial)];
    end
```

Esse diagrama mostra o processo de preparaÃ§Ã£o dos dados para que possam ser consultados depois.

1. **CÃ³digo Fonte / Documentos:** Ã‰ o ponto de partida. Representa toda a sua base de conhecimento bruta que vocÃª deseja que a IA consulte.

2. **Quebrar em PedaÃ§os (Chunks):** Os documentos ou cÃ³digos sÃ£o divididos em segmentos menores e lÃ³gicos. Isso Ã© importante porque facilita para o modelo de IA entender o contexto de cada pedaÃ§o individualmente.

3. **Modelo de Embedding (IA):** Cada "chunk" de texto Ã© entÃ£o processado por um modelo de inteligÃªncia artificial especÃ­fico (o Embedding Model). A funÃ§Ã£o deste modelo Ã© transformar o significado semÃ¢ntico do texto em vetor.

4. **Banco de Dados Vetorial:** Por fim, tanto os pedaÃ§os de texto originais quanto seus respectivos vetores sÃ£o armazenados no banco de dados. Um banco que tenha a engine de busca com base em similaridade de vetores.

### Conversando com o agente com RAG

Do ponto de vista do usuÃ¡rio/consumidor, o processo do RAG pode ser dividido em duas etapas.

1. **RecuperaÃ§Ã£o (Retrieval):** Quando vocÃª faz uma pergunta a um sistema com RAG, ele primeiro busca as informaÃ§Ãµes mais relevantes para a sua questÃ£o em uma base de dados especÃ­fica. Essa base pode ser qualquer coisa: documentos internos de uma empresa, artigos de notÃ­cias recentes, manuais de produtos, etc.

2. **GeraÃ§Ã£o (Generation):** Com essa "cola" em mÃ£os - ou seja, as informaÃ§Ãµes recuperadas -, a InteligÃªncia Artificial gera uma resposta muito mais completa, precisa e contextualizada para a sua pergunta. Ela combina o que encontrou na busca com seu conhecimento geral para formular a melhor resposta possÃ­vel.

#### Fluxo Completo

```mermaid
graph TD
    A[ğŸ‘¨ UsuÃ¡rio] -->|1 - Faz uma pergunta| B[ğŸ¤– Sistema RAG]
    B -->|2 - Usa a pergunta para buscar| C[ğŸ“š Base de Conhecimento]
    C -->|3 - Documentos Internos, PDFs, Manuais, etc.| Cdesc[(Base de Dados)]
    C -->| 4 - Encontra trechos relevantes| B
    B -->| 5 - Envia Pergunta + Trechos Relevantes| D[ğŸ§  Modelo de Linguagem LLM]
    D -->| 6 - Gera uma resposta contextualizada| B
    B -->| 7 - Entrega a resposta final| A
```

1. **A Pergunta (UsuÃ¡rio -> Sistema RAG)** - Tudo comeÃ§a com o prompt, como "Preciso de um crud de usuÃ¡rios com as propriedades name, email, password, lastLogin, createdAt, updatedAt, status (ativo | inativo)".

2. **A Busca (Sistema RAG -> Base de Conhecimento)** - Em vez de tentar adivinhar a resposta, o sistema RAG primeiro atua como um motor de busca. Ele vasculha a Base de Conhecimento (seus documentos ou cÃ³digos) para encontrar trechos exatos que falam sobre "crud".

3. **A "Cola" (Base de Conhecimento -> Sistema RAG)** - O sistema coleta os `chunks` que encontrar, como cÃ³digos com comentÃ¡rios explicando a implementaÃ§Ã£o do crud juntamente com exemplos.

4. **O Contexto (Sistema RAG -> LLM)** - Com os chunks, o sistema agora envia para o LLM um prompt com sua mensagem juntamente Ã  "cola" encontrada.

5. **A Resposta (LLM -> Sistema RAG)** - O LLM lÃª a pergunta e usa as informaÃ§Ãµes da "cola" como referÃªncia.

6. **A Entrega (Sistema RAG -> UsuÃ¡rio)** - O sistema entrega a resposta final e bem fundamentada para o usuÃ¡rio.