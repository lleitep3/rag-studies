
# RAG Code Assistant â€” RepositÃ³rio de Estudos

> **Este repositÃ³rio Ã© dedicado a estudos, experimentaÃ§Ã£o e aprendizado sobre sistemas RAG (Retrieval-Augmented Generation) aplicados Ã  anÃ¡lise inteligente de cÃ³digo Python.**

Aqui vocÃª encontrarÃ¡ exemplos, protÃ³tipos, testes de conceitos e implementaÃ§Ãµes didÃ¡ticas, sem compromisso com produÃ§Ã£o ou estabilidade. O objetivo Ã© explorar ideias, aprender e compartilhar conhecimento sobre RAG, LLMs, busca semÃ¢ntica e arquitetura de sistemas inteligentes.

---


## ğŸ¯ **Sobre o Projeto**

Este projeto Ã© um assistente de cÃ³digo inteligente que:
- ğŸ“– **LÃª e entende** cÃ³digo Python
- ğŸ¤” **Responde perguntas** sobre arquitetura e implementaÃ§Ã£o
- ğŸ” **Busca contextualmente** em toda a base de cÃ³digo
- ğŸ’¬ **Conversa naturalmente** sobre desenvolvimento
- ğŸ–¥ï¸ **Roda localmente** sem dependÃªncias externas

> **AtenÃ§Ã£o:** Este projeto Ã© experimental e serve para fins de estudo. Sinta-se Ã  vontade para explorar, modificar e contribuir com novas ideias!


## ğŸš€ **Como comeÃ§ar**

### Demo Interativa
```bash
# Clone e entre no projeto
git clone <repo-url>
cd rag-study

# Instale dependÃªncias
pip install -r requirements.txt

# Execute o chat interativo
cd apps/interactive-chat
python main.py
```

**Exemplos de perguntas:**
- "Como funciona o PythonLoader?"
- "DiferenÃ§a entre GeminiLLM e OllamaLLM?"
- "Como adicionar um novo loader?"
- "Como usar o RAGEngine?"


## ğŸ“‹ **PrÃ©-requisitos**

### ObrigatÃ³rios
- **Python 3.8+**
- **Ollama** instalado e rodando
  ```bash
  # Instalar Ollama
  curl -fsSL https://ollama.ai/install.sh | sh

  # Baixar modelo (recomendado)
  ollama pull llama3.2:1b

  # Iniciar servidor
  ollama serve
  ```

### Opcionais (para funcionalidades completas)
- **Google AI API Key** (para Gemini)
- **ChromaDB embeddings** (para vector store completo)


## ğŸ—ï¸ **Arquitetura**

```
src/
â”œâ”€â”€ ğŸ“ loaders/        # Carregamento de documentos
â”œâ”€â”€ ğŸ§  llms/           # Modelos de linguagem (Gemini, Ollama)
â”œâ”€â”€ ğŸ—‚ï¸ vector_stores/  # Armazenamento vetorial (ChromaDB)
â””â”€â”€ âš™ï¸ core/          # RAGEngine - Orquestrador principal
```

**PadrÃµes de Design:**
- ğŸ­ **Factory Pattern** - CriaÃ§Ã£o flexÃ­vel de componentes
- ğŸ”§ **Strategy Pattern** - MÃºltiplas estratÃ©gias de busca
- ğŸ“¦ **Dependency Injection** - Baixo acoplamento


## ğŸ’¡ **Funcionalidades**

### âœ… **Implementado**
- ğŸ **PythonLoader** - Carregamento inteligente de cÃ³digo
- ğŸ¤– **MÃºltiplos LLMs** - Gemini (remoto) + Ollama (local)
- ğŸ” **Vector Store** - ChromaDB com embeddings
- ğŸ¯ **RAG Engine** - Pipeline completo
- ğŸ’¬ **Chat Interativo** - Demo funcional

### ğŸ”§ **ConfiguraÃ§Ãµes**
- ğŸ“ MÃºltiplos tipos de busca (similarity, MMR, threshold)
- ğŸ›ï¸ ParÃ¢metros ajustÃ¡veis (chunk size, temperatura)
- ğŸ”„ Troca dinÃ¢mica de modelos
- ğŸ’¾ PersistÃªncia automÃ¡tica


## ğŸ› ï¸ **InstalaÃ§Ã£o**

### 1. Clone o repositÃ³rio
```bash
git clone <repo-url>
cd rag-study
```

### 2. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 3. Configure o ambiente (opcional)
```bash
# Para usar Gemini (opcional)
cp .env.example .env
# Edite .env com sua GOOGLE_API_KEY
```

### 4. Instale e configure Ollama
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo leve (recomendado)
ollama pull llama3.2:1b

# Ou modelo maior (melhor qualidade)
ollama pull gemma3:4b

# Iniciar servidor
ollama serve
```


## ğŸ® **Como usar**

### ğŸš€ **Aplicativos**
O projeto inclui vÃ¡rios aplicativos organizados em [`apps/`](apps/):

```bash
# Chat interativo inteligente
cd apps/interactive-chat
python main.py
```

ğŸ“± **[Ver todos os aplicativos disponÃ­veis â†’](apps/README.md)**

### Uso ProgramÃ¡tico
```python
from src.core.engine import create_rag_engine

# Criar sistema RAG
engine = create_rag_engine(
    loader_type="python",
    vector_store_type="chroma",
    llm_type="ollama",
    llm_kwargs={"model_name": "llama3.2:1b"}
)

# Indexar cÃ³digo
stats = engine.setup_pipeline("./src", "**/*.py")

# Fazer perguntas
result = engine.ask("Como funciona o PythonLoader?")
print(result['answer'])
```


## ğŸ“š **DocumentaÃ§Ã£o**

- ğŸ“– **[Conceitos RAG](docs/rag-concepts.md)** - Teoria e fundamentos
- ğŸ—ï¸ **[Arquitetura](docs/architecture.md)** - Design e componentes
- ğŸ”§ **[ConfiguraÃ§Ã£o](config.yaml)** - ParÃ¢metros do sistema


## ğŸ¤ **Contribuindo**

### Estrutura para novos componentes
```python
# Novo Loader
class MyLoader(BaseLoader):
    def load(self, path: str, glob: str) -> List[Document]:
        # ImplementaÃ§Ã£o
        pass

# Registrar na factory
# src/loaders/__init__.py
if loader_type == "my_type":
    return MyLoader(**kwargs)
```


## ğŸ”¬ **Tecnologias**

- ğŸ **Python 3.8+**
- ğŸ¦œ **LangChain** - Framework RAG
- ğŸŸ¢ **Ollama** - LLMs locais
- ğŸ’ **Google Gemini** - LLM remoto
- ğŸ¨ **ChromaDB** - Vector database
- âš¡ **FastAPI** - API framework (futuro)


## ğŸ“Š **Status**

```
âœ… Core System        - 100% funcional
âœ… Demo Interactive   - Completa
âœ… Local LLMs        - Ollama integrado
âœ… Code Analysis     - Funcionando
ğŸ”„ Config System     - Em desenvolvimento
ğŸ”„ Advanced Logging  - Planejado
```

---


---

**Este repositÃ³rio Ã© para fins de estudo e experimentaÃ§Ã£o. NÃ£o hÃ¡ garantias de funcionamento em produÃ§Ã£o. ContribuiÃ§Ãµes, sugestÃµes e dÃºvidas sÃ£o bem-vindas!**

**Experimente agora:** `cd apps/interactive-chat && python main.py`
